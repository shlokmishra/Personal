{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BCS_Task.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "d31XFJMeWcgP"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        " \n",
        "#Defining some functions\n",
        "def sigmoid(z):\n",
        "    return 1.0/(1.0+np.exp(-z))\n",
        " \n",
        "def sigmoid_prime(z):\n",
        "    return sigmoid(z)*(1-sigmoid(z))\n",
        " \n",
        "class Network:\n",
        "    # sizes is a list of the number of nodes in each layer\n",
        "    def init(self, sizes):\n",
        "        self.num_layers = len(sizes)\n",
        "        self.sizes = sizes\n",
        "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
        "        self.weights = [np.random.randn(y, x) for x,y in zip(sizes[:-1], sizes[1:])]\n",
        "       \n",
        "    def feedforward(self, a):\n",
        "        for b, w in zip(self.biases, self.weights):\n",
        "            a = sigmoid(np.dot(w, a) + b)\n",
        "        return a\n",
        "   \n",
        "    def SGD(self, training_data, epochs, mini_batch_size, eta, test_data=None):\n",
        "        training_data = list(training_data)\n",
        "        samples = len(training_data)\n",
        "       \n",
        "        if test_data:\n",
        "            test_data = list(test_data)\n",
        "            n_test = len(test_data)\n",
        "       \n",
        "        for j in range(epochs):\n",
        "            random.shuffle(training_data)\n",
        "            mini_batches = [training_data[k:k+mini_batch_size]\n",
        "                            for k in range(0, samples, mini_batch_size)]\n",
        "            for mini_batch in mini_batches:\n",
        "                self.update_mini_batch(mini_batch, eta)\n",
        "            if test_data:\n",
        "                print(f\"Epoch {j}: {self.evaluate(test_data)} / {n_test}\")\n",
        "            else:\n",
        "                print(f\"Epoch {j} complete\")\n",
        "   \n",
        "    def cost_derivative(self, output_activations, y):\n",
        "        return(output_activations - y)\n",
        "   \n",
        "    def backprop(self, x, y):\n",
        "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
        "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
        "        # feedforward\n",
        "        activation = x\n",
        "        activations = [x] # stores activations layer by layer\n",
        "        zs = [] # stores z vectors layer by layer\n",
        "        for b, w in zip(self.biases, self.weights):\n",
        "            z = np.dot(w, activation) + b\n",
        "            zs.append(z)\n",
        "            activation = sigmoid(z)\n",
        "            activations.append(activation)\n",
        "       \n",
        "        # backward pass\n",
        "        delta = self.cost_derivative(activations[-1], y) * sigmoid_prime(zs[-1])\n",
        "        nabla_b[-1] = delta\n",
        "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
        "       \n",
        "        for _layer in range(2, self.num_layers):\n",
        "            z = zs[-_layer]\n",
        "            sp = sigmoid_prime(z)\n",
        "            delta = np.dot(self.weights[-_layer+1].transpose(), delta) * sp\n",
        "            nabla_b[-_layer] = delta\n",
        "            nabla_w[-_layer] = np.dot(delta, activations[-_layer-1].transpose())\n",
        "        return (nabla_b, nabla_w)\n",
        "   \n",
        "    def update_mini_batch(self, mini_batch, eta):\n",
        "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
        "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
        "        for x, y in mini_batch:\n",
        "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
        "            nabla_b = [nb + dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
        "            nabla_w = [nw + dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
        "        self.weights = [w-(eta/len(mini_batch))*nw\n",
        "                        for w, nw in zip(self.weights, nabla_w)]\n",
        "        self.biases = [b-(eta/len(mini_batch))*nb\n",
        "                       for b, nb in zip(self.biases, nabla_b)]\n",
        "       \n",
        "    def evaluate(self, test_data):\n",
        "        test_results = [(np.argmax(self.feedforward(x)), y)\n",
        "                        for (x, y) in test_data]\n",
        "        return sum(int(y[x]) for (x, y) in test_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We start with importing numpy and random libraries. Random library is used to randomize the starting weights in our neural network while numpy (or np in our code) helps us make the calculations faster.\n",
        "We then define two popular helper functions, which are sigmoid and sigmoid prime. Signmoid prime is the derivative which is used in backpropagation to calculate the gradient.\n",
        "\n",
        "# ***IN THE NETWORK:***\n",
        "*   **sizes variable**: List of numbers that indicates the number of input nodes at each layer in our neural network.\n",
        "*   **init function**: Four attributes are initialized:\n",
        "\n",
        "    *   number of layers, num_layers, is set to the length of the sizes\n",
        "    *   list of the sizes of the layers is set to the input variables, sizes\n",
        "    *   initial biases of our network are randomized for each layer after the input layer\n",
        "    *   weights connecting each node are randomized for each connection between the input and output layers\n",
        "\n",
        "\n",
        "\n",
        "(*np.random.randn() returns a random sample from the normal distribution*)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "*   **feedforward function**: Sends information forward in the neural network. This function will take one parameter, â€˜aâ€™, which represents the current activation vector. It loops through all the biases and weights and calculates the activations at each layer. The 'a' returned is the predeiction (activations of last layer).\n",
        "\n",
        "*   **gradient descent function**: Four mandatory parameters and one optional parameter. \n",
        "\n",
        "    *   set of training data\n",
        "    *   number of epochs\n",
        "    *   sizeo of the mini-batches\n",
        "    *   learning rate (eta)\n",
        "    *   test data (optional)\n",
        "\n",
        "\n",
        "> Converts the training_data into a list type and sets the number of samples to the length of that list (Same is done to the test data). This is because these are not returned to us as lists, but zips of lists. Now, we loop through the number of training epochs,wherein each epoch, we start by shuffling the data (for randomness), and create a list of mini-batches.The update_mini_batch function is called for each mini batch  (Test accuracy is also returned if test data is present).\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "*   **cost derivative function:** Determines if we made a mistake in our output layer. It takes two parameters:\n",
        "\n",
        "    *   output_activations array\n",
        "    *   expected output values, y\n",
        "\n",
        "\n",
        "\n",
        "Now for backpropagation, the backprop function will take two values, x, and y. We initialize our nablas (ð›) {gradients} to 0 vectors. We also need to keep track of our current activation vector, activation, all of the activation vectors, activations (the input layer being the first one), and the z-vectors, zs.\n",
        "\n",
        "Now, weâ€™ll loop through all the biases and weights. In each loop we calculate the z vector as the dot product of the weights and activation, add that to the list of zs, recalculate the activation, and then add the new activation to the list of activations.\n",
        "\n",
        "*   **backpropagation function:** We calculate the delta, which is error from the last layer multiplied by the sigmoid_prime of the last entry of the zs vectors. We set the last layer of nabla_b as the delta and the last layer of nabla_w equal to the dot product of the delta and the second to last layer of activations (transposed). We do the same thing for each layer going backwards starting from the second to last layer. Finally, we return the nablas as a tuple.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "*   **update mini batch function:**   It starts pretty much the same way as our backprop function by creating 0 vectors of the nablas for the biases and weights. It takes two parameters:\n",
        "  \n",
        "  *   mini_batch\n",
        "  *   earning rate, eta\n",
        "\n",
        "\n",
        "\n",
        "> Then, for each input, x, and output, y, in the mini_batch, we get the delta of each nabla array (by backprop function). Next, we update the nabla lists with these deltas. Finally, we update the weights and biases of the network using the nablas and the learning rate. Each value is updated to the current value minus the learning rate divided by the size of the minibatch times the nabla value.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "*   **evaluate funtion:** This function takes one parameter, the test_data.The networkâ€™s outputs (which are calculated by feeding forward the input, x) are simply compared with the expected output, y.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xBPGvWkBWvTs"
      }
    }
  ]
}